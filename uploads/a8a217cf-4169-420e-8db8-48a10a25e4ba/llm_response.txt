{
    "code": "import pandas as pd\nimport requests\nimport os\nimport io\n\n# Create the directory if it doesn't exist\noutput_dir = \"uploads/a8a217cf-4169-420e-8db8-48a10a25e4ba\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Define file paths\ndata_file_path = os.path.join(output_dir, \"data.csv\")\nmetadata_file_path = os.path.join(output_dir, \"metadata.txt\")\n\n# URL to scrape\nurl = \"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"\n\n# Fetch HTML content\nresponse = requests.get(url)\nresponse.raise_for_status() # Raise an exception for HTTP errors\n\n# Read HTML tables into a list of DataFrames\ntables = pd.read_html(response.text)\n\n# The main table of highest-grossing films is usually the first or second one.\n# Find the table with 'Film' and 'Worldwide gross' columns.\ndf = None\nfor table in tables:\n    if 'Film' in table.columns and 'Worldwide gross' in table.columns:\n        df = table\n        break\n\nif df is None:\n    raise ValueError(\"Could not find the expected table on the Wikipedia page.\")\n\n# Rename 'Worldwide gross' for easier access\ndf = df.rename(columns={'Worldwide gross': 'Worldwide_gross'})\n\n# Clean 'Rank' column: extract numeric part from potential footnotes/superscripts\ndf['Rank'] = df['Rank'].astype(str).str.extract(r'^(\\d+)', expand=False)\ndf['Rank'] = pd.to_numeric(df['Rank'], errors='coerce')\n\n# Clean 'Worldwide_gross' column: remove '$', ',', and bracketed notes like '[1]', then convert to numeric\ndf['Worldwide_gross'] = df['Worldwide_gross'].astype(str).str.replace('$', '', regex=False).str.replace(',', '', regex=False).str.replace(r'\\[.*?\\]', '', regex=True).str.strip()\ndf['Worldwide_gross'] = pd.to_numeric(df['Worldwide_gross'], errors='coerce')\n\n# Clean 'Year' column: extract the first four digits, convert to numeric\ndf['Year'] = df['Year'].astype(str).str.extract(r'(\\d{4})', expand=False)\ndf['Year'] = pd.to_numeric(df['Year'], errors='coerce')\n\n# Drop rows with NaN in critical columns after cleaning\ndf.dropna(subset=['Rank', 'Film', 'Worldwide_gross', 'Year'], inplace=True)\n\n# Select final columns for the dataset\n# Ensure 'Rank', 'Film', 'Worldwide_gross', 'Year' are present.\nfinal_df = df[['Rank', 'Film', 'Worldwide_gross', 'Year']].copy()\n\n# Save the cleaned DataFrame to CSV\nfinal_df.to_csv(data_file_path, index=False)\n\n# Generate metadata.txt\nwith open(metadata_file_path, 'w') as f:\n    f.write(f\"Data File Path: {data_file_path}\\n\\n\")\n    f.write(\"--- df.info() ---\\n\")\n    # Redirect df.info() output to a string\n    buffer = io.StringIO()\n    final_df.info(buf=buffer)\n    f.write(buffer.getvalue())\n    f.write(\"\\n\\n--- Column Names ---\\n\")\n    f.write(\", \".join(final_df.columns) + \"\\n\\n\")\n    f.write(\"--- df.head() ---\\n\")\n    f.write(final_df.head().to_string() + \"\\n\\n\")\n    f.write(\"--- Notes ---\\n\")\n    f.write(\"The original Wikipedia table does not contain a column explicitly named 'Peak'.\\n\")\n    f.write(\"For the purpose of answering the user's questions regarding 'Peak',\\n\")\n    f.write(\"the 'Worldwide_gross' column has been used as an equivalent for 'Peak',\\n\")\n    f.write(\"as it represents the maximum grossing value for each film.\\n\\n\")\n    f.write(\"ANSWER_FORMAT: JSON\\n\")\n    f.write(\"[\")\n    f.write(\"\\n  \\\"Answer to Q1\\\",\")\n    f.write(\"\\n  \\\"Answer to Q2\\\",\")\n    f.write(\"\\n  \\\"Answer to Q3\\\",\")\n    f.write(\"\\n  \\\"Answer to Q4 (base64 image data URI)\\\"\\n\")\n    f.write(\"]\\n\")",
    "libraries": [
        "pandas",
        "requests",
        "lxml",
        "beautifulsoup4"
    ],
    "questions": [
        "How many $2 bn movies were released before 2000?",
        "Which is the earliest film that grossed over $1.5 bn?",
        "What's the correlation between the Rank and Peak?",
        "Draw a scatterplot of Rank and Peak along with a dotted red regression line through it."
    ],
    "comment": "Step-3: Getting scrap code and metadata from llm. Tries count = %d 0"
}{
    "code": "import pandas as pd\nimport os\n\n# Define URL and paths\nurl = \"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"\noutput_dir = \"uploads/a8a217cf-4169-420e-8db8-48a10a25e4ba\"\ndata_path = os.path.join(output_dir, \"data.csv\")\nmetadata_path = os.path.join(output_dir, \"metadata.txt\")\n\n# Create output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# Scrape the tables\ntables = pd.read_html(url)\ndf = None\n# Iterate through tables to find the one with 'Film', 'Worldwide gross ($)', 'Year', 'Rank', 'Peak'\nexpected_columns_subset = ['Film', 'Worldwide gross ($)', 'Year', 'Rank', 'Peak']\n\nfor table in tables:\n    # Normalize column names by stripping whitespace and replacing non-breaking spaces\n    original_cols = table.columns\n    normalized_cols = [c.replace('\\xa0', ' ').strip() if isinstance(c, str) else c for c in original_cols]\n    table.columns = normalized_cols\n\n    # Check if all expected columns (or their normalized versions) are present\n    if all(col in table.columns for col in expected_columns_subset):\n        df = table\n        break\n\nif df is None:\n    raise ValueError(\"Could not find the expected table on the Wikipedia page with columns 'Film', 'Worldwide gross ($)', 'Year', 'Rank', 'Peak'.\")\n\n# --- Data Cleaning ---\n\n# Clean 'Worldwide gross ($)' column: remove currency symbols, commas, references, convert to numeric (in dollars)\nif 'Worldwide gross ($)' in df.columns:\n    df['Worldwide gross'] = df['Worldwide gross ($)'].astype(str).str.replace('$', '', regex=False).str.replace(',', '', regex=False)\n    df['Worldwide gross'] = df['Worldwide gross'].str.replace(r'\\[\\d+\\]', '', regex=True).str.strip()\n    # Handle ranges like \"1.1\u20131.2\" by taking the first value. Convert to float.\n    df['Worldwide gross'] = df['Worldwide gross'].apply(lambda x: float(x.split('\u2013')[0].split('-')[0]) if isinstance(x, str) and x else None)\n    # Drop the original column\n    df.drop(columns=['Worldwide gross ($)'], inplace=True)\nelif 'Worldwide gross' in df.columns: # Fallback if name is already simplified\n    df['Worldwide gross'] = df['Worldwide gross'].astype(str).str.replace('$', '', regex=False).str.replace(',', '', regex=False)\n    df['Worldwide gross'] = df['Worldwide gross'].str.replace(r'\\[\\d+\\]', '', regex=True).str.strip()\n    df['Worldwide gross'] = df['Worldwide gross'].apply(lambda x: float(x.split('\u2013')[0].split('-')[0]) if isinstance(x, str) and x else None)\n\n\n# Clean 'Year' column: extract 4-digit year and convert to numeric\nif 'Year' in df.columns:\n    # Extract the first 4-digit number (year) from the string, then convert to numeric\n    df['Year'] = pd.to_numeric(df['Year'].astype(str).str.extract(r'(\\d{4})')[0], errors='coerce')\n\n# Convert 'Rank' and 'Peak' to numeric\nif 'Rank' in df.columns:\n    df['Rank'] = pd.to_numeric(df['Rank'].astype(str).str.replace(r'\\[\\d+\\]', '', regex=True), errors='coerce')\n\nif 'Peak' in df.columns:\n    df['Peak'] = pd.to_numeric(df['Peak'].astype(str).str.replace(r'\\[\\d+\\]', '', regex=True), errors='coerce')\n\n\n# Save the DataFrame to CSV\ndf.to_csv(data_path, index=False)\n\n# Generate metadata\nwith open(metadata_path, \"w\") as f:\n    f.write(f\"Data file path: {data_path}\\n\")\n    f.write(f\"Description: Highest-grossing films data from Wikipedia\\n\\n\")\n    f.write(\"df.info():\\n\")\n    df.info(buf=f)\n    f.write(\"\\nColumn Names:\\n\")\n    f.write(\", \".join(df.columns.tolist()))\n    f.write(\"\\n\\nFirst few rows (df.head()):\\n\")\n    f.write(df.head().to_string())\n    f.write(\"\\n\\nANSWER_FORMAT: JSON array of strings\")",
    "libraries": [
        "pandas"
    ],
    "questions": [
        "How many $2 bn movies were released before 2000?",
        "Which is the earliest film that grossed over $1.5 bn?",
        "What's the correlation between the Rank and Peak?",
        "Draw a scatterplot of Rank and Peak along with a dotted red regression line through it. Return as a base-64 encoded data URI, \"data:image/png;base64,iVBORw0G...\" under 100,000 bytes."
    ],
    "comment": "Step-4: Error occured while scrapping. Tries count = %d, 0"
}{
    "code": "import pandas as pd\nimport os\nimport io\n\n# Define output directory and file paths\noutput_dir = 'uploads/a8a217cf-4169-420e-8db8-48a10a25e4ba'\ndata_file_path = os.path.join(output_dir, 'data.csv')\nmetadata_file_path = os.path.join(output_dir, 'metadata.txt')\n\n# Create the output directory if it does not exist\nos.makedirs(output_dir, exist_ok=True)\n\n# URL to scrape\nurl = 'https://en.wikipedia.org/wiki/List_of_highest-grossing_films'\n\n# Required columns as specified in the error message for table identification\nrequired_columns = ['Film', 'Worldwide gross ($)', 'Year', 'Rank', 'Peak']\n\ndf_found = None\n\ntry:\n    # Read all tables from the Wikipedia page\n    dfs = pd.read_html(url)\n\n    # Iterate through the tables to find the one with all required columns\n    for df in dfs:\n        # Normalize column names to check for required_columns\n        # Remove any leading/trailing spaces or special characters for robust matching if necessary\n        # For now, stick to exact match based on the error message.\n        if all(col in df.columns for col in required_columns):\n            df_found = df\n            break\n\n    if df_found is None:\n        raise ValueError(f\"Could not find the expected table on the Wikipedia page with columns {required_columns}.\")\n\n    # Save the identified DataFrame to CSV\n    df_found.to_csv(data_file_path, index=False)\n\n    # Generate metadata.txt\n    with open(metadata_file_path, 'w') as f:\n        f.write(f\"Data File Path: {data_file_path}\\n\\n\")\n\n        f.write(\"df.info() output:\\n\")\n        buffer = io.StringIO()\n        df_found.info(buf=buffer)\n        f.write(buffer.getvalue())\n        f.write(\"\\n\")\n\n        f.write(\"Column Names:\\n\")\n        f.write(str(df_found.columns.tolist()))\n        f.write(\"\\n\\n\")\n\n        f.write(\"df.head() output:\\n\")\n        f.write(df_found.head().to_string())\n        f.write(\"\\n\\n\")\n\n        f.write(\"ANSWER_FORMAT: JSON\")\n\nexcept Exception as e:\n    # Log the error if necessary or print for debugging\n    print(f\"An error occurred during data collection: {e}\")\n    raise\n",
    "libraries": [
        "pandas"
    ],
    "questions": [
        "How many $2 bn movies were released before 2000?",
        "Which is the earliest film that grossed over $1.5 bn?",
        "What's the correlation between the Rank and Peak?",
        "Draw a scatterplot of Rank and Peak along with a dotted red regression line through it. Return as a base-64 encoded data URI, \"data:image/png;base64,iVBORw0B...\" under 100,000 bytes."
    ],
    "comment": "Step-4: Error occured while scrapping. Tries count = %d, 1"
}{
    "code": "import pandas as pd\nimport os\nimport io\n\n# Define paths\noutput_dir = \"uploads/a8a217cf-4169-420e-8db8-48a10a25e4ba\"\ndata_file_path = os.path.join(output_dir, \"data.csv\")\nmetadata_file_path = os.path.join(output_dir, \"metadata.txt\")\n\n# Create the output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# URL of the Wikipedia page\nurl = \"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"\n\n# Expected columns to identify the correct table\nexpected_columns = ['Film', 'Worldwide gross ($)', 'Year', 'Rank', 'Peak']\n\ntry:\n    # Read all tables from the Wikipedia page\n    tables = pd.read_html(url)\nexcept Exception as e:\n    raise RuntimeError(f\"Failed to read HTML from {url}: {e}\")\n\n# Find the table that contains all expected columns\ndf = None\nfor table in tables:\n    # Normalize column names for robust matching (remove extra spaces, footnotes etc.)\n    normalized_table_columns = [col.split('[')[0].strip() for col in table.columns]\n    \n    # Check if all expected columns are present in the table's normalized columns\n    if set(expected_columns).issubset(set(normalized_table_columns)):\n        df = table\n        break\n\nif df is None:\n    raise ValueError(f\"Could not find the expected table on the Wikipedia page with columns {expected_columns}.\")\n\n# Standardize column names (remove footnotes from original names)\ndf.columns = [col.split('[')[0].strip() for col in df.columns]\n\n# Data cleaning\n# 'Worldwide gross ($)' column: remove '$', commas, 'approx.', convert to numeric\nif 'Worldwide gross ($)' in df.columns:\n    df['Worldwide gross ($)'] = df['Worldwide gross ($)'].astype(str).str.replace('$', '', regex=False).str.replace(',', '', regex=False).str.replace('approx\\.', '', regex=True).str.strip()\n    # Handle ranges like \"1,000\u20131,100\" by taking the first number, or empty strings\n    df['Worldwide gross ($)'] = df['Worldwide gross ($)'].apply(lambda x: x.split('\u2013')[0] if '\u2013' in x else x)\n    df['Worldwide gross ($)'] = pd.to_numeric(df['Worldwide gross ($)'], errors='coerce')\n    # Convert from millions of dollars to absolute dollars (e.g., 2923.7 million -> 2,923,700,000)\n    df['Worldwide gross ($)'] = df['Worldwide gross ($)'] * 1_000_000\n\n# 'Year' column: convert to numeric, handling potential footnotes/ranges\nif 'Year' in df.columns:\n    df['Year'] = df['Year'].astype(str).str.split('[').str[0].str.strip()\n    df['Year'] = pd.to_numeric(df['Year'], errors='coerce')\n\n# 'Rank' column: convert to numeric\nif 'Rank' in df.columns:\n    df['Rank'] = df['Rank'].astype(str).str.split('[').str[0].str.strip()\n    df['Rank'] = df['Rank'].replace('\u2014', pd.NA)\n    df['Rank'] = pd.to_numeric(df['Rank'], errors='coerce')\n\n# 'Peak' column: convert to numeric\nif 'Peak' in df.columns:\n    df['Peak'] = df['Peak'].astype(str).str.split('[').str[0].str.strip()\n    df['Peak'] = df['Peak'].replace('\u2014', pd.NA)\n    df['Peak'] = pd.to_numeric(df['Peak'], errors='coerce')\n\n# Select only the relevant columns to save, and ensure their order\nfinal_df = df[expected_columns].copy()\n\n# Save the DataFrame to CSV\nfinal_df.to_csv(data_file_path, index=False)\n\n# Generate metadata\nwith open(metadata_file_path, \"w\") as f:\n    f.write(f\"Data File Path: {data_file_path}\\n\")\n    f.write(\"Short Description: Highest-grossing films from Wikipedia.\\n\\n\")\n\n    # Capture df.info() output\n    buffer = io.StringIO()\n    final_df.info(buf=buffer)\n    f.write(\"DataFrame Info:\\n\")\n    f.write(buffer.getvalue())\n    f.write(\"\\n\")\n\n    f.write(\"Column Names:\\n\")\n    for col in final_df.columns:\n        f.write(f\"- {col}\\n\")\n    f.write(\"\\n\")\n\n    f.write(\"First Few Rows (df.head()):\\n\")\n    f.write(final_df.head().to_string())\n    f.write(\"\\n\\n\")\n    \n    f.write(\"ANSWER_FORMAT: JSON array of strings containing the answer.\\n\")\n",
    "libraries": [
        "pandas"
    ],
    "questions": [
        "How many $2 bn movies were released before 2000?",
        "Which is the earliest film that grossed over $1.5 bn?",
        "What's the correlation between the Rank and Peak?",
        "Draw a scatterplot of Rank and Peak along with a dotted red regression line through it. Return as a base-64 encoded data URI, \"data:image/png;base64,iVBORw0PCM...\" under 100,000 bytes."
    ],
    "comment": "Step-4: Error occured while scrapping. Tries count = %d, 2"
}